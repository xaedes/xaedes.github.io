<!DOCTYPE html>
<html>
<title>Kalman-Filter</title>
<link rel="stylesheet" href="lib/highlight-styles/vs.css">


<xmp theme="spacelab" style="display:none;" toc="true" >

[Back to index](index.html)


# Kalman-Filter (KF)

State is modelled with normal distributions.
State transition over time is modelled as linear operations on state and normal distributed control variables.
Observations are incorporated in the state estimation with a linear model linking observations and state.

## system model

| Symbol          | Shape                      | Description                                      |
| -------         | ---                        | ------------                                     |
| $\pmb{x}$       | $\mathbb{R}^{N \times 1}$  | system state vector                              |
| $\hat{\pmb{x}}$ | $\mathbb{R}^{N \times 1}$  | state estimation                                 |
| $\pmb{P}$       | $\mathbb{R}^{N \times N}$  | state estimation error covariance matrix         |
| $\pmb{F}$       | $\mathbb{R}^{N \times N}$  | system dynamic state transition matrix           |
| $\pmb{Q}$       | $\mathbb{R}^{N \times N}$  | prediction covariance                            |
| $\pmb{u}$       | $\mathbb{R}^{K \times 1}$  | external system input vector (e.g. user control) |
| $\pmb{B}$       | $\mathbb{R}^{N \times K}$  | external input state dynamic                     |
| $\pmb{z}$       | $\mathbb{R}^{M \times 1}$  | observation vector                               |
| $\pmb{H}$       | $\mathbb{R}^{N \times M}$  | observation matrix transforms a state vector to a observation vector |
| $\pmb{R}$       | $\mathbb{R}^{M \times M}$  | observation error covariance matrix              |


$N$ number of states

$M$ number of observations

$K$ number of external system inputs (often controls)

<br/ >

The process model describes how the state varies over time.

$\textbf{process model:} (\pmb{F}, \pmb{B}, \pmb{x}_k, \pmb{P}_k, \pmb{u}_k) \mapsto (\pmb{x}_{k+1}) $

$\pmb{x}_{k+1} =  \pmb{F} \cdot \pmb{x}_k + \pmb{B} \cdot \pmb{u}_k + \pmb{v} $

$\pmb{v}$ is zero mean normal distributed noise.

$\pmb{v} \sim N(0,\pmb{Q})$


[Input control noise is encoded in prediction covariance.](https://stats.stackexchange.com/questions/134920/kalman-filter-with-input-control-noise)
The prediction covariance $\pmb{Q}$ contains state transition covariance $\pmb{Q}_{F}$ and external input dynamic covariance $\pmb{Q}_{B}$:

$\pmb{v}_{F} \sim N(0,\pmb{Q}_{F})$

$\pmb{v}_{B} \sim N(0,\pmb{Q}_{B})$

$\pmb{v} = \pmb{v}_{F} + \pmb{v}_{B}$

$\pmb{Q} = \pmb{Q}_{F} + \pmb{B} \cdot \pmb{Q}_{B} \cdot \pmb{B}^T$

$\pmb{Q}_{F} \in \mathbb{R}^{N \times N}$, $\pmb{Q}_{B} \in \mathbb{R}^{N \times K}$

$\pmb{x}_{k+1} =  \pmb{F} \cdot \pmb{x}_k + \pmb{v}_{F} + \pmb{B} \cdot \pmb{u}_k + \pmb{v}_{B} $

<br/ >

The observation model describes an observation in terms of the state:

$\textbf{observation model:}$ $(\pmb{H}, \pmb{x}_k) \mapsto (\widetilde{\pmb{z}}_{k}) $

$\widetilde{\pmb{z}}_{k} = \pmb{H} \cdot \pmb{x}_{k} + \pmb{u}$

$\pmb{u}$ models zero mean normal distributed measurement noise.

$\pmb{u} \sim N(0,\pmb{R})$

<br/ >

## filter equations
A kalman filter is a 7-tuple with transition matrices, their covariance matrices, an estimated state and its covariance matrix:

$\textbf{KalmanFilter:}$ $(\pmb{F}, \pmb{B}, \pmb{Q}, \pmb{H}, \pmb{R}, \hat{\pmb{x}}, \pmb{P})$

<br/ >

__NOTE__: prediction and observation here happen at different steps $k$. The time associated with each step can be different, but strictly monotous increasing with $k$. This makes it easy to use the same equations for equally spaced timesteps with simple predict & observe iterations on measurement as well as for varying timestep and interleaved predict and observe operations.

<br/ >

$\textbf{initial condition:}$ $(\pmb{x}_0, \pmb{P}_0)$

<br/ >

$\textbf{predict:}$ $(\pmb{F}, \pmb{B}, \hat{\pmb{x}_k}, \pmb{P}_k, \pmb{u}_k) \mapsto (\hat{\pmb{x}}_{k+1}, \pmb{P}_{k+1}) $

$\hat{\pmb{x}}_{k+1} = \pmb{F} \cdot \hat{\pmb{x}_k} + \pmb{B} \cdot \pmb{u}_k$

$\pmb{P}_{k+1} = \pmb{F} \cdot \pmb{P}_k \cdot \pmb{F}^T + \pmb{Q}$

<br/ >
$\textbf{observe:}$ $(\pmb{H}, \hat{\pmb{x}_k}, \pmb{P}_k, \pmb{z}_k) \mapsto (\hat{\pmb{x}}_{k+1}, \pmb{P}_{k+1}) $

innovation: 
$\pmb{w} = \pmb{z}_k - \pmb{H} \cdot \hat{\pmb{x}}_k$

$\pmb{w} \in \mathbb{R}^{M \times 1}$

residual covariance:
$\pmb{S} = \pmb{H} \cdot \pmb{P} \cdot \pmb{H}^T + \pmb{R}$

$\pmb{S} \in \mathbb{R}^{M \times M}$

kalman gain:
$\pmb{K} = \pmb{P} \cdot \pmb{H}^T \cdot \pmb{S}^{-1}$

$\pmb{K} = \frac{\pmb{P} \cdot \pmb{H}^T}{\pmb{S}}$

$\pmb{K} = \frac{\pmb{P} \cdot \pmb{H}^T}{\pmb{H} \cdot \pmb{P} \cdot \pmb{H}^T + \pmb{R}}$ weights the measurement covariance with the current state estimation covariance.

$\pmb{K} \in \mathbb{R}^{N \times M}$

$\hat{\pmb{x}}_{k+1} = \hat{\pmb{x}}_k + \pmb{K} \cdot \pmb{w}$

$\pmb{P}_{k+1} = \pmb{P}_k - \pmb{K} \cdot \pmb{S} \cdot \pmb{K}^T$

<br/ >

Due to numerical errors the computation of $\pmb{P}_{k+1}$ can result in invalid non-symmetrical covariance matrices.
To restore symmetry:

$\textbf{symmetrize:}$ $P_{symmetrical} = \frac{P + P^T}{2}$




## web links

- http://bilgin.esme.org/BitsAndBytes/KalmanFilterforDummies
- https://github.com/xaedes/Python-Extendend-Kalman-Filter/blob/master/Kalman.py
- https://stats.stackexchange.com/questions/134920/kalman-filter-with-input-control-noise
- https://en.wikipedia.org/wiki/Kalman_filter

german links:
- https://de.wikipedia.org/wiki/Kalman-Filter
- http://www.cbcity.de/das-kalman-filter-einfach-erklaert-teil-1
- https://www.ei.rub.de/media/ei/lehrmaterialien/120/a896a6a0b135530a8a00b094fa631b2ea0e1c95d/KalmanFolien.pdf

## Examples

### Accumulate Simple Observations

We directly observe the state. The state is assumed to not change at all.
We just want to accumulate observations to improve the state estimation. 
No real time is involved here only computation time steps $k$. 



### Simple State Tracking with Observations

We directly observe the state. We model the state process as a random walk with movements drawn from $N(0,\pmb{Q})$. 
Since we have $N = 1$, $M = 1$ all matrices collapse to singular values.
There are no external inputs so all terms related to that are omitted.

$\pmb{F} = 1$ State stays constant
$\pmb{H} = 1$ Direct measurement of state


Prediction:

$\hat{\pmb{x}}_{k+1} = \hat{\pmb{x}_k}$

$\pmb{P}_{k+1} = \pmb{P}_k + \pmb{Q}$

Observation:

innovation: 
$\pmb{w} = \pmb{z}_k - \hat{\pmb{x}}_k$

residual covariance:
$\pmb{S} = \pmb{P} + \pmb{R}$

kalman gain:
$\pmb{K} = \pmb{P} \cdot \pmb{S}^{-1} = \frac{\pmb{P}}{\pmb{P} + \pmb{R}}$

$\hat{\pmb{x}}_{k+1} = \hat{\pmb{x}}_k + \pmb{K} \cdot \pmb{w}$

$\hat{\pmb{x}}_{k+1} = \hat{\pmb{x}}_k + \frac{\pmb{P}}{\pmb{P} + \pmb{R}} \cdot \pmb{w}$

$\hat{\pmb{x}}_{k+1} = \hat{\pmb{x}}_k + \frac{\pmb{P}_k}{\pmb{P}_k + \pmb{R}} \cdot (\pmb{z}_k - \hat{\pmb{x}}_k)$

$\hat{\pmb{x}}_{k+1} = \hat{\pmb{x}}_k + \frac{\pmb{P}_k}{\pmb{P}_k + \pmb{R}} \cdot \pmb{z}_k - \frac{\pmb{P}_k}{\pmb{P}_k + \pmb{R}} \cdot \hat{\pmb{x}}_k$

$\hat{\pmb{x}}_{k+1} = \hat{\pmb{x}}_k \cdot (1 - \frac{\pmb{P}_k}{\pmb{P}_k + \pmb{R}}) + \frac{\pmb{P}_k}{\pmb{P}_k + \pmb{R}} \cdot \pmb{z}_k$

$\hat{\pmb{x}}_{k+1} = \hat{\pmb{x}}_k \cdot (1 - \pmb{K}) + \pmb{K} \cdot \pmb{z}_k$

$\pmb{P}_{k+1} = \pmb{P}_k - \pmb{K} \cdot \pmb{S} \cdot \pmb{K}^T$

$\pmb{P}_{k+1} = \pmb{P}_k - \frac{\pmb{P}_k^2}{\pmb{P}_k + \pmb{R}}$

$\pmb{P}_{k+1} = \pmb{P}_k - \pmb{P}_k \cdot \pmb{K}$

$\pmb{P}_{k+1} = \pmb{P}_k \cdot (1 - \pmb{K})$

$\pmb{P}$ converges to $\pmb{P}_{\text{conv}}$ for constant $\pmb{Q}$ and $\pmb{R}$.

$\pmb{P}_{\text{conv}} = (\pmb{P}_{\text{conv}} + \pmb{Q}) \cdot (1 - \frac{\pmb{P}_{\text{conv}} + \pmb{Q}}{\pmb{P}_{\text{conv}} + \pmb{Q} + \pmb{R}}) $

$\pmb{P}_{\text{conv}} = \pmb{P}_{\text{conv}} - \frac{\pmb{P}_{\text{conv}}^2 + \pmb{P}_{\text{conv}} \cdot \pmb{Q}}{\pmb{P}_{\text{conv}} + \pmb{Q} + \pmb{R}} + \pmb{Q} - \frac{\pmb{Q} \cdot \pmb{P}_{\text{conv}} + \pmb{Q}^2}{\pmb{P}_{\text{conv}} + \pmb{Q} + \pmb{R}}$

$\pmb{P}_{\text{conv}} = \pmb{P}_{\text{conv}} + \pmb{Q} - \frac{\pmb{P}_{\text{conv}}^2 + 2 \pmb{Q} \cdot \pmb{P}_{\text{conv}} + \pmb{Q}^2}{\pmb{P}_{\text{conv}} + \pmb{Q} + \pmb{R}}$

$\pmb{Q} = \frac{\pmb{P}_{\text{conv}}^2 + 2 \pmb{Q} \cdot \pmb{P}_{\text{conv}} + \pmb{Q}^2}{\pmb{P}_{\text{conv}} + \pmb{Q} + \pmb{R}}$

$\pmb{Q} \cdot \pmb{P}_{\text{conv}} + \pmb{Q}^2 + \pmb{Q} \cdot \pmb{R} = \pmb{P}_{\text{conv}}^2 + 2 \pmb{Q} \cdot \pmb{P}_{\text{conv}} + \pmb{Q}^2$

$\pmb{P}_{\text{conv}}^2 + \pmb{Q} \cdot \pmb{P}_{\text{conv}} - \pmb{Q} \cdot \pmb{R} = 0$

Quadratic formula:
$\pmb{P}_{\text{conv}} = -\frac{\pmb{Q}}{2} + \sqrt{\frac{\pmb{Q}^2}{4} + \pmb{Q} \cdot \pmb{R}}$

### 

# Extended-Kalman-Filter (EKF)



# Information-Filter (IF)

State estimation uncertainty $\pmb{P}$ is replaced by information matrix $\pmb{Y}$:

$\pmb{Y} = \pmb{P}^{-1}$

State estimation $\pmb{x}$ is replaced by information vector $\pmb{y}$:

$\pmb{y} = \pmb{P}^{-1} \cdot \pmb{x}$

Prediction:

$\pmb{Y}_k = (\pmb{F} \cdot \pmb{Y}^{-1}_{k-1} \cdot \pmb{F}^T + \pmb{Q})^-1$


$\pmb{Y}_k = (\pmb{F}^T + \pmb{Y}_{k-1} \cdot \pmb{F}^{-1} \cdot \pmb{Q})^-1 \cdot \pmb{Y}_{k-1} \cdot \pmb{F}^{-1}$ Alternative formulation avoiding inverse of $\pmb{Y}$.

$\pmb{y}_k = \pmb{Y}_k \cdot (\pmb{F} \cdot \pmb{Y}^{-1}_{k-1} \cdot \pmb{y}_{k-1} + \pmb{B} \cdot \pmb{u})$

Observation:

$\pmb{Y}_k = \pmb{H}^T \cdot \pmb{R}^{-1} \cdot \pmb{H} + \pmb{Y}_{k-1}$

$\pmb{y}_k = \pmb{H}^T \cdot \pmb{R}^{-1} \cdot \pmb{z}_k + \pmb{y}_{k-1}$

## Examples

### Simple State Tracking with Observations

All matrices are $\in \mathbb{R}^{1 \times 1} $, so we simplify them to scalars $\in \mathbb{R}$:

$\pmb{F} = F = 1$

$\pmb{H} = H = 1$

$\pmb{B} = B = 0$

$\pmb{R} = R$, 
$\pmb{Q} = Q$,
$\pmb{Y} = Y$,
$\pmb{y} = y$,
$\pmb{x} = x$,
$\pmb{z} = z$

Prediction:

$Y_k = (1 + Y_{k-1} \cdot Q)^{-1} \cdot Y_{k-1}$

...

Observation:

...

## Weblinks

- https://en.wikipedia.org/wiki/Kalman_filter#Information_filter
- https://cse.sc.edu/~terejanu/files/tutorialKF.pdf 
- http://ais.informatik.uni-freiburg.de/teaching/ws12/mapping/pdf/slam06-eif.pdf 
- http://ais.informatik.uni-freiburg.de/teaching/ws13/mapping/pdf/slam07-eif.pdf 

[Back to index](index.html) &raquo;
[Back to top](#) 

</xmp>

<script src="lib/strapdown-zeta/strapdown.min.js"></script>
</html>
